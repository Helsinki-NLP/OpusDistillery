####
# Example of a production config
# Change language pair, experiment name, datasets and other settings if needed
# Training low resource languages might require more tuning of pipeline/training/configs
###

experiment:
  dirname: nllb
  name: test
  langpairs:
    - en-et

  split-length: 1000

  best-model: perplexity
  
  huggingface:
    modelname: "facebook/nllb-200-distilled-600M"
    modelclass: "transformers.AutoModelForSeq2SeqLM"
    lang_info: True # you need to check if the model requires src and target langs or not
    lang_tags:
      en: eng_Latn
      et: est_Latn

  spm-vocab-size: 2880

marian-args:
  training-student:
    disp-freq: 10
    save-freq: 100
    valid-freq: 100
    after: 500u
  training-student-finetuned:
    disp-freq: 10
    save-freq: 100
    valid-freq: 100
    after: 500u

datasets:
  train:
    - opus_ELRC_2922__v1
  devtest:
    - flores_dev
  test:
    - flores_devtest
